{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informational-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timeit\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "# ./indexer\n",
    "from indexer import AppendIndexer\n",
    "import ALS\n",
    "\n",
    "# Annoy\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "#SKLearn \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# scipy\n",
    "from scipy.spatial import distance\n",
    "from scipy.sparse import lil_matrix\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secondary-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewMatrix:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.original = True\n",
    "        self.item_indexer = AppendIndexer.load('./chetor.com/view_matrix/item_indexer.indexer')\n",
    "        self.user_indexer = AppendIndexer.load('./chetor.com/view_matrix/user_indexer.indexer')\n",
    "        \n",
    "    def load_matrix(path):\n",
    "        \n",
    "        matrix = ViewMatrix(path)\n",
    "\n",
    "        try:\n",
    "            matrix.view_matrix = ViewMatrix \\\n",
    "                .load_sparse_lil(path)\n",
    "        except:\n",
    "            print('Error: loading', path)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def load_sparse_lil(filename):\n",
    "        loader = np.load(filename, allow_pickle=True)\n",
    "        result = lil_matrix(tuple(loader[\"shape\"]), dtype=str(loader[\"dtype\"]))\n",
    "        result.data = loader[\"data\"]\n",
    "        result.rows = loader[\"rows\"]\n",
    "        return result\n",
    "    \n",
    "    def make_dense(self, user_min_view, item_min_view):\n",
    "        self.original = False\n",
    "        while True:\n",
    "            removed_rows_cnt = self.trim_users_with_few_views(user_min_view)\n",
    "            removed_columns_cnt = self.trim_columns_with_few_views(item_min_view)\n",
    "            if not removed_columns_cnt and not removed_rows_cnt:\n",
    "                break\n",
    "\n",
    "    def trim_users_with_few_views(self, user_min_view):\n",
    "        removing_row_indices = list(np.where(self.view_matrix.getnnz(1) < user_min_view)[0])\n",
    "        print('Number of users which should be deleted:', len(removing_row_indices))\n",
    "        self.trim_user_indices(to_remove_indices=removing_row_indices)\n",
    "        return len(removing_row_indices)\n",
    "\n",
    "    def trim_columns_with_few_views(self, column_min_view):\n",
    "        removing_column_indices = list(np.where(self.view_matrix.getnnz(0) < column_min_view)[0])\n",
    "        print('Number products which should be deleted:', len(removing_column_indices))\n",
    "        self.trim_column_indices(to_remove_indices=removing_column_indices)\n",
    "        return len(removing_column_indices)\n",
    "    \n",
    "    def trim_user_indices(self, to_remove_indices):\n",
    "        self.user_indexer.remove_indexes(to_remove_indices)\n",
    "        self.view_matrix = ViewMatrix.delete_row_lil(self.view_matrix, to_remove_indices)\n",
    "    \n",
    "    def trim_column_indices(self, to_remove_indices):\n",
    "        self.item_indexer.remove_indexes(to_remove_indices)\n",
    "        self.view_matrix = ViewMatrix.delete_column_lil(self.view_matrix, to_remove_indices)\n",
    "    \n",
    "    def delete_column_lil(mat: lil_matrix, *i) -> lil_matrix:\n",
    "        mat = mat.transpose()\n",
    "        mat = ViewMatrix.delete_row_lil(mat, *i)\n",
    "        return mat.transpose()\n",
    "    \n",
    "    def delete_row_lil(mat: lil_matrix, *i) -> lil_matrix:\n",
    "        if not isinstance(mat, lil_matrix):\n",
    "            raise ValueError(\"works only for LIL format -- use .tolil() first\")\n",
    "        mat = mat.copy()\n",
    "        mat.rows = np.delete(mat.rows, i)\n",
    "        mat.data = np.delete(mat.data, i)\n",
    "        mat._shape = (mat.rows.shape[0], mat._shape[1])\n",
    "        return mat\n",
    "    def to_csr(self):\n",
    "        train_data = self.view_matrix.astype(np.float64)\n",
    "        train_data = train_data.tocoo()\n",
    "        train_data.data = np.log10(train_data.data) + 1\n",
    "        train_data = train_data.tocsr()\n",
    "        return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorporated-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALSReady(path: str, l = 2):\n",
    "    now = time.time()\n",
    "    matrix = ViewMatrix.load_matrix(path)\n",
    "    print('View matrix loaded in', time.time() - now, 'seconds.')\n",
    "\n",
    "    now = time.time()\n",
    "    sparce_matrix = matrix.to_csr()\n",
    "    matrix.make_dense(user_min_view = l, \n",
    "                      item_min_view = l)\n",
    "    implicit_matrix = matrix.to_csr()\n",
    "    print('matrix has been made dense in', time.time() - now, 'seconds.')\n",
    "    return matrix, sparce_matrix, implicit_matrix\n",
    "\n",
    "def CFTrain(matrix, implicit_matrix, _alpha = 15, _facs = 20, _itr = 15, save = False):\n",
    "    \n",
    "    now = time.time()\n",
    "    als_model = ALS.Als(num_factors = _facs,\n",
    "                        iterations = _itr,\n",
    "                        num_threads = 10,\n",
    "                        alpha = _alpha)\n",
    "\n",
    "    \n",
    "    als_model.fit(implicit_matrix)\n",
    "    alsTime = time.time() - now\n",
    "    print('ALS model is fitted in', alsTime, 'seconds.')\n",
    "    if save:\n",
    "        print('Saving Data ...')\n",
    "        matrix.item_indexer.dump('./chetor.com/alisResult/ALS/ali_item_indexer_factorized.indexer')\n",
    "        matrix.user_indexer.dump('./chetor.com/alisResult/ALS/ali_user_indexer_factorized.indexer')\n",
    "        np.save('./chetor.com/alisResult/ALS/ali_items_vectors.npy', als_model.item_vectors)\n",
    "        np.save('./chetor.com/alisResult/ALS/ali_users_vectors.npy', als_model.user_vectors)\n",
    "\n",
    "    return als_model.item_vectors, als_model.user_vectors, alsTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beginning-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annoy_results(pages_vector, number_of_trees = 50, number_of_neighbours = 11):\n",
    "    \n",
    "    start = time.time()\n",
    "    annoy_model = annoy_model = AnnoyIndex(pages_vector.shape[1], 'angular')\n",
    "    \n",
    "    for index, vector in enumerate(pages_vector):\n",
    "        annoy_model.add_item(index, vector)\n",
    "    \n",
    "    annoy_model.build(number_of_trees)\n",
    "    \n",
    "    annoy_indices = []\n",
    "    annoy_distances = []\n",
    "\n",
    "    for i in pages_vector:\n",
    "        indices, distances = annoy_model. \\\n",
    "            get_nns_by_vector(i, number_of_neighbours, include_distances=True)\n",
    "        annoy_indices.append(indices)\n",
    "        annoy_distances.append(distances)\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    print('annoy time:', duration)\n",
    "    return annoy_indices, annoy_distances, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-aberdeen",
   "metadata": {},
   "source": [
    "implicit matrix ro migire va ye bakhshisho baramun test o train mikone ke ye bakhshi az cell ha sefr shodan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excess-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_set_precision_recall(implicit_matrix, _test_size = 0.05, test_cells = 0.2):\n",
    "    train, test = train_test_split(implicit_matrix, shuffle=False, test_size = _test_size)\n",
    "    print('test_shape', test.shape, 'train_shape (which we cant process bc of RAM)', train.shape)\n",
    "    rows,cols = test.nonzero()\n",
    "    delete_index = [(rows[i], cols[i]) for i in random.sample(range(1, len(rows)), int(len(rows)*test_cells))]\n",
    "    print(\"total cells\", len(rows), \"number of deleted cells\", len(delete_index))\n",
    "    \n",
    "    x_train = scipy.sparse.csr_matrix(test.shape)\n",
    "    for i, j in tqdm(zip(rows, cols)):\n",
    "        if (i, j) in delete_index:\n",
    "            continue\n",
    "        x_train[i, j] = test[i, j]\n",
    "    return x_train, test, delete_index\n",
    "\n",
    "def prepare_data_for_AP(implicit_matrix, _test_size = 0.05):\n",
    "    train, test = train_test_split(implicit_matrix, shuffle=False, test_size = _test_size)\n",
    "    print('test_shape', test.shape, 'train_shape (which we cant process bc of RAM)', train.shape)\n",
    "    rows, cols = test.nonzero()\n",
    "    unique, counts = np.unique(rows, return_counts=True)\n",
    "    commulative_count = [0]\n",
    "    for i in counts:\n",
    "        commulative_count.append(commulative_count[-1] + i)\n",
    "    delete_row = []\n",
    "    for i in range(1, len(commulative_count)):\n",
    "        delete_row.append(np.random.randint(commulative_count[i-1], commulative_count[i]))\n",
    "    delete_index = [(rows[i], cols[i]) for i in delete_row]\n",
    "    # print(delete_row[:10], delete_index[:10], rows[:10], cols[:10], random.randrange(commulative_count[0], 10, 2) )\n",
    "    print(\"total cells\", len(rows), \"number of deleted cells\", len(delete_index))\n",
    "    \n",
    "    x_train = scipy.sparse.csr_matrix(test.shape)\n",
    "    for i, j in zip(rows, cols):\n",
    "        if (i, j) in delete_index:\n",
    "            continue\n",
    "        x_train[i, j] = test[i, j]\n",
    "    return x_train, test, delete_index\n",
    "\n",
    "def prepare_data_for_MAE(implicit_matrix, set_size = 0.05):\n",
    "    train, test = train_test_split(implicit_matrix, shuffle=False, test_size = set_size)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enormous-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate_at_k(deleted, x_train, test_approx, k = 10):\n",
    "    found = []\n",
    "    for i in tqdm(range(len(test_approx))):\n",
    "        ta = list(n_argmax(test_approx[i], 30))\n",
    "        nonz = np.nonzero(x_train[i])[0]\n",
    "        same_old = []\n",
    "        for l in range(len(ta)):\n",
    "            if ta[l] in nonz:\n",
    "                same_old.append(l)\n",
    "\n",
    "        for l in same_old[::-1]:\n",
    "            ta.pop(l)\n",
    "        ta = ta[:k]        \n",
    "        for j in ta:\n",
    "            found.append((i, j))\n",
    "    same = 0\n",
    "    for i in tqdm(deleted):\n",
    "        if i in found:\n",
    "            same += 1\n",
    "    print(same, len(deleted), same/len(deleted))\n",
    "    return same/len(deleted)\n",
    "\n",
    "def visited_at_k(x_train, test_approx, k = 10):\n",
    "    same_old = []\n",
    "    for i in tqdm(range(len(test_approx))):\n",
    "        ta = list(n_argmax(test_approx[i], k))\n",
    "        nonz = np.nonzero(x_train[i])[0]\n",
    "        for l in range(len(ta)):\n",
    "            if ta[l] in nonz:\n",
    "                same_old.append(l)\n",
    "    rows,cols = x_train.nonzero()\n",
    "    return len(same_old)/len(rows)\n",
    "\n",
    "def n_argmax(a, n):\n",
    "    ranked = np.argsort(a)\n",
    "    largest_indices = ranked[::-1][:n]\n",
    "    return largest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subsequent-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random search\n",
    "itrs1 = [15, 15, 15, 15]\n",
    "alphas1 = [5, 5, 5, 5]\n",
    "factors1 = [5, 10, 20, 40]\n",
    "\n",
    "itrs2 = [15, 15, 15, 15]\n",
    "alphas2 = [10, 10, 10, 10]\n",
    "factors2 = [5, 10, 20, 40]\n",
    "\n",
    "itrs3 = [15, 15, 15, 15]\n",
    "alphas3 = [50, 50, 50, 50]\n",
    "factors3 = [5, 10, 20, 40]\n",
    "\n",
    "itrs4 = [15, 15, 15, 15]\n",
    "alphas4 = [100, 100, 100, 100]\n",
    "factors4 = [5, 10, 20, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-benchmark",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-edwards",
   "metadata": {},
   "source": [
    "accuracy metrics:\n",
    "- recall@k, hit_rate@k\n",
    "- visited_rate@k\n",
    "- MAE on train set per iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-occasions",
   "metadata": {},
   "source": [
    "time metrics:\n",
    "- training time\n",
    "- query time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-jerusalem",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "hit_rate@k, visited@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chief-sweet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View matrix loaded in 7.075235843658447 seconds.\n",
      "Number of users which should be deleted: 1863687\n",
      "Number products which should be deleted: 1308\n",
      "Number of users which should be deleted: 265\n",
      "Number products which should be deleted: 4\n",
      "Number of users which should be deleted: 1\n",
      "Number products which should be deleted: 1\n",
      "Number of users which should be deleted: 0\n",
      "Number products which should be deleted: 0\n",
      "matrix has been made dense in 17.982833862304688 seconds.\n"
     ]
    }
   ],
   "source": [
    "matrix, sparce_matrix, implicit_matrix = ALSReady('./chetor.com/view_matrix/lil_matrix.npz', l=2)\n",
    "# print(implicit_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brazilian-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/yektanet/.local/lib/python3.8/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "89it [00:00, 888.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_shape (16324, 6877) train_shape (which we cant process bc of RAM) (216873, 6877)\n",
      "total cells 39169 number of deleted cells 7833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39169it [00:43, 904.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape: (16324, 6877) \n",
      "test set shape: (16324, 6877) \n",
      "deleted cells: 7833 <class 'scipy.sparse.csr.csr_matrix'> <class 'scipy.sparse.csr.csr_matrix'>\n",
      "ALS model is fitted in 1.3875703811645508 seconds.\n",
      "(6877, 20) (16324, 20)\n",
      "(16324, 6877) (16324, 6877)\n"
     ]
    }
   ],
   "source": [
    "#to test\n",
    "x_train, test, deleted = generate_test_set_precision_recall(implicit_matrix, _test_size = 0.07)\n",
    "print('train set shape:', x_train.shape, '\\ntest set shape:', test.shape, '\\ndeleted cells:', len(deleted), type(test), type(x_train))\n",
    "#training ALS model\n",
    "item_vectors, user_vectors, alsTime = \\\n",
    "CFTrain(None, x_train, _alpha = 10, _facs = 20, _itr = 20)\n",
    "print(item_vectors.shape, user_vectors.shape)\n",
    "test_approx = np.matmul(user_vectors, item_vectors.T)\n",
    "print(test_approx.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "linear-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hit_rate_at_k(deleted, x_train.toarray(), test_approx, k = 6)\n",
    "# hit_rate_at_k(deleted, x_train.toarray(), test_approx, k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "labeled-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visited_at_k(x_train.toarray(), test_approx, k = 6)\n",
    "# visited_at_k(x_train.toarray(), test_approx, k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mighty-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_visited_hitRate(alphas, itrs, factors, _l = 2, data_set_size = 0.072, log = False):\n",
    "    visited3s = []\n",
    "    hitRate3s = []\n",
    "    visited6s = []\n",
    "    hitRate6s = []\n",
    "    AP3s = []\n",
    "    AP6s = []\n",
    "    trainingTime = []\n",
    "    qTime = []\n",
    "    matrix, sparce_matrix, implicit_matrix = ALSReady('./chetor.com/view_matrix/lil_matrix.npz', l=_l)\n",
    "    x_train_AP, test_AP, delete_index_AP = prepare_data_for_AP(implicit_matrix, _test_size = data_set_size)\n",
    "    x_train, test, deleted = generate_test_set_precision_recall(implicit_matrix, _test_size = data_set_size)\n",
    "    for a,i,f in zip(alphas, itrs, factors):\n",
    "        if log: print('------alphas, itrs, factors------', a,i,f)\n",
    "        item_vectors, user_vectors, alsTime = \\\n",
    "        CFTrain(None, x_train, _alpha = a, _facs = f, _itr = i)\n",
    "        a1, a2, duration = get_annoy_results(item_vectors)\n",
    "        qTime.append(duration)\n",
    "        trainingTime.append(alsTime)\n",
    "        if log: print('training and querry time:', trainingTime[-1], qTime[-1])\n",
    "        test_approx = np.matmul(user_vectors, item_vectors.T)\n",
    "        visited3s.append(visited_at_k(x_train.toarray(), test_approx, k = 3))\n",
    "        if log: print('visited3s', visited3s[-1])\n",
    "        visited6s.append(visited_at_k(x_train.toarray(), test_approx, k = 6))\n",
    "        if log: print('visited6s', visited6s[-1])\n",
    "        hitRate3s.append(hit_rate_at_k(deleted, x_train.toarray(), test_approx, k = 3))\n",
    "        if log: print('hitRate3s', hitRate3s[-1])\n",
    "        hitRate6s.append(hit_rate_at_k(deleted, x_train.toarray(), test_approx, k = 6))\n",
    "        if log: print('hitRate6s', hitRate6s[-1])\n",
    "        \n",
    "        item_vectors_AP, user_vectors_AP, alsTime_AP = \\\n",
    "        CFTrain(None, x_train_AP, _alpha = a, _facs = f, _itr = i)\n",
    "        test_approx_AP = np.matmul(user_vectors_AP, item_vectors_AP.T)\n",
    "        \n",
    "        AP3s.append(hit_rate_at_k(delete_index_AP, x_train_AP, test_approx_AP, k = 3))\n",
    "        if log: print('AP3s', AP3s[-1])\n",
    "        AP6s.append(hit_rate_at_k(delete_index_AP, x_train_AP, test_approx_AP, k = 6))\n",
    "        if log: print('AP6s', AP6s[-1])\n",
    "    return visited3s, visited6s, hitRate3s, hitRate6s, AP3s, AP6s, trainingTime, qTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-helena",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------\n",
    "Average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "decent-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix, sparce_matrix, implicit_matrix_AP = ALSReady('./chetor.com/view_matrix/lil_matrix.npz', l=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "charming-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing\n",
    "# x_train_AP, test_AP, delete_index_AP = prepare_data_for_AP(implicit_matrix_AP, _test_size = 0.2)\n",
    "# #training ALS model\n",
    "# item_vectors_AP, user_vectors_AP, alsTime_AP = \\\n",
    "# CFTrain(None, x_train_AP, _alpha = 10, _facs = 20, _itr = 20)\n",
    "# print(item_vectors_AP.shape, user_vectors_AP.shape)\n",
    "# test_approx_AP = np.matmul(user_vectors_AP, item_vectors_AP.T)\n",
    "# print(test_approx_AP.shape, test_AP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "diverse-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"average precision k = 3\", hit_rate_at_k(delete_index_AP, x_train_AP, test_approx_AP, k = 3))\n",
    "# print(\"average precision k = 6\", hit_rate_at_k(delete_index_AP, x_train_AP, test_approx_AP, k = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "running-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_AP(alphas, itrs, factors, _l = 3, data_set_size = 0.2):\n",
    "    AP3s = []\n",
    "    AP6s = []\n",
    "    time\n",
    "    matrix, sparce_matrix, implicit_matrix_AP = ALSReady('./chetor.com/view_matrix/lil_matrix.npz', l=_l)\n",
    "    x_train_AP, test_AP, delete_index_AP = prepare_data_for_AP(implicit_matrix_AP, _test_size = data_set_size)\n",
    "    for a,i,f in zip(alphas, itrs, factors):\n",
    "        item_vectors_AP, user_vectors_AP, alsTime_AP = \\\n",
    "        CFTrain(None, x_train_AP, _alpha = a, _facs = f, _itr = i)\n",
    "        test_approx_AP = np.matmul(user_vectors_AP, item_vectors_AP.T)\n",
    "        AP3s.append(hit_rate_at_k(delete_index_AP, x_train_AP, test_approx_AP, k = 3))\n",
    "        AP6s.append(hit_rate_at_k(delete_index_AP, x_train_AP, test_approx_AP, k = 6))\n",
    "    return AP3s, AP6s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-parameter",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------\n",
    "MAE per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monthly-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix, sparce_matrix, implicit_matrix = ALSReady('./chetor.com/view_matrix/lil_matrix.npz', l=2)\n",
    "# train = prepare_data_for_MAE(implicit_matrix, set_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coral-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE_calc(train, itr = 10, facs = 20):\n",
    "    item_vectors, user_vectors, alsTime = \\\n",
    "    CFTrain(None, train, _alpha = 10, _facs = 20, _itr = itr)\n",
    "    approximated_matrix = np.matmul(user_vectors, item_vectors.T)\n",
    "    absolute_error = np.absolute(train - approximated_matrix)\n",
    "    return np.mean(absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "registered-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAEs = []\n",
    "# for i in [2,5,7,10,15,20,30,40,80,100]:\n",
    "#     MAEs.append(MAE_calc(train, itr = i))\n",
    "#     print(MAEs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "oriented-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(19), MAEs[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-factor",
   "metadata": {},
   "source": [
    "## Run on all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited3s1, visited6s1, hitRate3s1, hitRate6s1, AP3s1, AP6s1, trainingTime1, qTime1 = \\\n",
    "calc_visited_hitRate(alphas1, itrs1, factors1, _l = 2, data_set_size = 0.03, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited3s2, visited6s2, hitRate3s2, hitRate6s2, AP3s2, AP6s2, trainingTime2, qTime2 = \\\n",
    "calc_visited_hitRate(alphas2, itrs2, factors2, _l = 2, data_set_size = 0.03, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited3s3, visited6s3, hitRate3s3, hitRate6s3, AP3s3, AP6s3, trainingTime3, qTime3 = \\\n",
    "calc_visited_hitRate(alphas3, itrs3, factors3, _l = 2, data_set_size = 0.03, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited3s4, visited6s4, hitRate3s4, hitRate6s4, AP3s4, AP6s4, trainingTime4, qTime4 = \\\n",
    "calc_visited_hitRate(alphas4, itrs4, factors4, _l = 2, data_set_size = 0.03, log = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
